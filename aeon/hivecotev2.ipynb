{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install aeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aeon.classification.hybrid import HIVECOTEV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from aeon.classification.hybrid import HIVECOTEV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'sensordata.ACC', 'sensordata.GYR', 'sensordata.GRV',\n",
      "       'sensordata.MAG', 'sensordata.ROT', 'duration'],\n",
      "      dtype='object', name='stype')\n",
      "Dataset shapes: \n",
      "(332, 800, 17)\n",
      "(332,)\n",
      "(332,)\n",
      "(2656, 100, 17) (2656,)\n",
      "train-test shapes: \n",
      "(232, 100, 17) (100, 100, 17) (232,) (100,)\n",
      "(232, 100, 17) (100, 100, 17) (232,) (100,)\n",
      "(2324, 100, 17) (2324,)\n"
     ]
    }
   ],
   "source": [
    "file_path = '../combined_all_interpol_synced_4sec_800samples_prot4.pkl'\n",
    "\n",
    "dataFrame = pd.read_pickle(file_path)\n",
    "\n",
    "\n",
    "print(dataFrame.columns)\n",
    "y = np.array(dataFrame['uid'])\n",
    "x = None\n",
    "for col in dataFrame.columns:\n",
    "    if 'sensordata' in col:\n",
    "        if x is None:\n",
    "            x = np.array(dataFrame[col].apply(lambda x: np.array(x)).to_list())\n",
    "            #print(x_data.shape)\n",
    "        else:\n",
    "            x_data = np.array(dataFrame[col].apply(lambda x: np.array(x)).to_list())\n",
    "            #print(x_data.shape)\n",
    "            x = np.append(x, x_data , axis=2)\n",
    "dur = np.array(dataFrame['duration'])\n",
    "print(\"Dataset shapes: \")\n",
    "print(x.shape)\n",
    "print(dur.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "def split_features_and_adjust_labels(x, y, window_size=100, overlap=0.4):\n",
    "    num_samples = x.shape[0]\n",
    "    num_timesteps = x.shape[1]\n",
    "    num_features = x.shape[2]\n",
    "\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    num_segments = int((num_timesteps - window_size) / step_size) + 1\n",
    "\n",
    "    split_x = np.zeros((num_samples * num_segments, window_size, num_features))\n",
    "    adjusted_y = np.repeat(y, num_segments)\n",
    "\n",
    "    segment_index = 0\n",
    "    for sample_index in range(num_samples):\n",
    "        for i in range(0, num_timesteps - window_size + 1, step_size):\n",
    "          split_x[segment_index] = x[sample_index, i : i + window_size, :]\n",
    "          segment_index += 1\n",
    "\n",
    "    return split_x, adjusted_y, num_segments\n",
    "\n",
    "def get_aligned_and_rest(split_x, adjusted_y,num_segments):\n",
    "    x_aligned = []\n",
    "    y_aligned = []\n",
    "    x_other = []\n",
    "    y_other = []\n",
    "\n",
    "    for i in range(len(adjusted_y)):\n",
    "        if i % num_segments == 0:\n",
    "            x_aligned.append(split_x[i])\n",
    "            y_aligned.append(adjusted_y[i])\n",
    "        else:\n",
    "            x_other.append(split_x[i])\n",
    "            y_other.append(adjusted_y[i])\n",
    "    return np.array(x_aligned), np.array(y_aligned), np.array(x_other), np.array(y_other)\n",
    "\n",
    "\n",
    "split_x, adjusted_y,num_segments  = split_features_and_adjust_labels(x,y,window_size=100,overlap=0)\n",
    "print(split_x.shape, adjusted_y.shape)\n",
    "x_aligned, y_aligned, x_other, y_other = get_aligned_and_rest(split_x,adjusted_y, num_segments)\n",
    "\n",
    "\n",
    "y_aligned = y_aligned.astype(str)\n",
    "y_other = y_other.astype(str)\n",
    "x = x_aligned\n",
    "y = y_aligned\n",
    "\n",
    "\n",
    "class_label_encoder = LabelEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y = class_label_encoder.fit_transform(y)\n",
    "y_other = class_label_encoder.transform(y_other)\n",
    "n_classes = class_label_encoder.classes_.shape[0]\n",
    "\n",
    "print(\"train-test shapes: \")\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n",
    "x_test = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "x_other_scaled = scaler.transform(x_other.reshape(-1, x_other.shape[-1])).reshape(x_other.shape)\n",
    "\n",
    "\n",
    "print(x_other_scaled.shape, y_other.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STC  14:35:19 10/12/2024\n",
      "STC weight = 0.6096260332179014\n",
      "DrCIF  14:39:28 10/12/2024\n",
      "DrCIF weight = 0.6713132812930787\n",
      "Arsenal  14:39:33 10/12/2024\n",
      "Arsenal weight = 0.5747163311417245\n",
      "TDE  14:58:33 10/12/2024\n",
      "TDE weight = 0.6096260332179014\n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "  # Fit HC2\n",
    "  hc2 = HIVECOTEV2(verbose=1,n_jobs = -1, )\n",
    "  hc2.fit(x_train, y_train)\n",
    "\n",
    "  # Predict and print accuracy\n",
    "  predictions = hc2.predict(x_test)\n",
    "  print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIVECOTEV2(n_jobs=-1, verbose=1)\n"
     ]
    }
   ],
   "source": [
    "with open('hc2_model.pkl', 'rb') as file:\n",
    "    hc2 = pickle.load(file)\n",
    "\n",
    "# Verify the loaded model\n",
    "print(hc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = hc2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        16\n",
      "           1       0.72      0.76      0.74        17\n",
      "           2       0.73      0.65      0.69        17\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       0.90      0.75      0.82        12\n",
      "           5       1.00      0.50      0.67         2\n",
      "           6       1.00      0.90      0.95        10\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       0.70      1.00      0.82         7\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.90      0.84      0.85       100\n",
      "weighted avg       0.84      0.83      0.83       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, pred))\n",
    "#print(sklearn.metrics.classification_report(y_other, predicted_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HIVECOTEV2' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhc2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HIVECOTEV2' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "print(hc2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hc2, open('hc2_model.pkl', 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
